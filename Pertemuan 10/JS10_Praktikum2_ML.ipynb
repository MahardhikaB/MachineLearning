{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "id": "initial_id"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Grc_Rly56Z4-",
        "outputId": "743da983-1526-464b-a865-3af0e94a13d5"
      },
      "id": "Grc_Rly56Z4-",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDIGKUyp5-qL",
        "outputId": "ef1c2c08-bd86-48db-afda-2c343367a26d"
      },
      "id": "yDIGKUyp5-qL",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPbft41q6BIi",
        "outputId": "5b283bd6-fbb5-4be2-e117-f740dde6fd01"
      },
      "id": "nPbft41q6BIi",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy7oXnTC6Dz3",
        "outputId": "3379ea30-b368-45f3-813a-a1be99b6b18b"
      },
      "id": "Jy7oXnTC6Dz3",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhMHYseU6llZ",
        "outputId": "64c28587-94d0-4e50-e5cf-2234b22ac1f2"
      },
      "id": "uhMHYseU6llZ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "mGeOELs263iX"
      },
      "id": "mGeOELs263iX",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khYMXilO6qCK",
        "outputId": "069309d5-c9a7-47df-ddb8-32b9cd5a1fa7"
      },
      "id": "khYMXilO6qCK",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "7gOaortn6uCk"
      },
      "id": "7gOaortn6uCk",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08Zs1Dfj7Lbe",
        "outputId": "a83a1487-ff06-4f1f-a10e-e6538d3a6577"
      },
      "id": "08Zs1Dfj7Lbe",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_4-qfPJ7NKL",
        "outputId": "13884b5a-943c-4cd1-fb87-1f6bbeed6d33"
      },
      "id": "j_4-qfPJ7NKL",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "wSIWFG7i7T5a"
      },
      "id": "wSIWFG7i7T5a",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czMzsivp7WTD",
        "outputId": "cb9dae3c-0f33-4219-d746-6533841073a4"
      },
      "id": "czMzsivp7WTD",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "p7l8UPac7eOo"
      },
      "id": "p7l8UPac7eOo",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upYofCXa7hPZ",
        "outputId": "dc5e28da-2eed-4ab0-93b2-53d0ed68d760"
      },
      "id": "upYofCXa7hPZ",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "Xaa6bkV27jzy"
      },
      "id": "Xaa6bkV27jzy",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItrrH9Jj7mqA",
        "outputId": "eac80706-1d08-4869-c0c4-bbbf7cb89b26"
      },
      "id": "ItrrH9Jj7mqA",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXXYwhmQ7oTQ",
        "outputId": "824c1dd7-9e62-4ad9-c8be-04cbfa383968"
      },
      "id": "lXXYwhmQ7oTQ",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        " input_text = sequence[:-1]\n",
        " target_text = sequence[1:]\n",
        " return input_text, target_text"
      ],
      "metadata": {
        "id": "KAN3x4Cn7qKB"
      },
      "id": "KAN3x4Cn7qKB",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUSKEUsI7yLU",
        "outputId": "c54d2e40-9912-4b5c-f3a8-bbc1864339cd"
      },
      "id": "qUSKEUsI7yLU",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "4BaOuN6D71Dh"
      },
      "id": "4BaOuN6D71Dh",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        " print(\"Input :\", text_from_ids(input_example).numpy())\n",
        " print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSVjaCvf73oY",
        "outputId": "b9af0dcd-7dea-41b2-d432-cf70893ddd9f"
      },
      "id": "PSVjaCvf73oY",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCHJW0ma76Xv",
        "outputId": "08002b02-241d-481b-9c9a-9b0522f48c37"
      },
      "id": "CCHJW0ma76Xv",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "PMi2CWmr7_pF"
      },
      "id": "PMi2CWmr7_pF",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "27Eo5fKf8BzT"
      },
      "id": "27Eo5fKf8BzT",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "nkVy21Ll8Dqp"
      },
      "id": "nkVy21Ll8Dqp",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9WpSgJN8E7e",
        "outputId": "093a985b-0578-403f-fb15-4ae4345ef23a"
      },
      "id": "y9WpSgJN8E7e",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl1nu1UV8GW0",
        "outputId": "53c6c3f0-4a2b-4914-d70f-23237050d540"
      },
      "id": "Pl1nu1UV8GW0",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "MmuMTDH38HXn"
      },
      "id": "MmuMTDH38HXn",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFfCkVP38Ikv",
        "outputId": "f1d6a2d1-b0d3-4fd4-8494-96fe5a680b5e"
      },
      "id": "OFfCkVP38Ikv",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([42, 47, 22, 41, 58, 45, 43, 54, 39, 14, 65, 18, 37, 55, 59, 64,  6,\n",
              "       31, 44,  4, 65, 39, 39, 10, 33, 36, 32, 23, 23,  7,  9,  9, 58, 12,\n",
              "       13, 17, 37, 42, 29, 23, 64,  3, 15, 16,  1,  9,  2, 37, 22,  3, 41,\n",
              "       59, 59, 59, 22, 37, 10,  7, 35, 54,  6, 54, 33,  6, 29, 14, 12, 63,\n",
              "       34, 31, 29, 30, 21, 54, 14,  7, 64, 20, 28,  3, 41, 59,  6, 28, 19,\n",
              "       29, 45, 31, 25, 42, 20, 26, 40, 60, 15, 33, 51, 37, 59, 23])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4puX63t8NDS",
        "outputId": "66e47ee1-0967-4438-b3a7-24cac94c4180"
      },
      "id": "k4puX63t8NDS",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'hy children mightily.\\n\\nLADY GREY:\\nHerein your highness wrongs both them and me.\\nBut, mighty lord, th'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"chIbsfdoZAzEXpty'Re$zZZ3TWSJJ,..s;?DXcPJy!BC\\n. XI!btttIX3,Vo'oT'PA;xURPQHoA,yGO!bt'OFPfRLcGMauBTlXtJ\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "zG-6GVfi8P3W"
      },
      "id": "zG-6GVfi8P3W",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJEl7a-z8TXE",
        "outputId": "66971f58-8be3-49aa-ee06-16d3d6b02477"
      },
      "id": "nJEl7a-z8TXE",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.188005, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nmo3HeYT8Wg0",
        "outputId": "bcc5e44c-8a1d-49e4-bd06-7192890b4ea9"
      },
      "id": "Nmo3HeYT8Wg0",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.891205"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "sxDMjeFe8X0s"
      },
      "id": "sxDMjeFe8X0s",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "q1a0hIUP8Y69"
      },
      "id": "q1a0hIUP8Y69",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "MHBNUruS8djv"
      },
      "id": "MHBNUruS8djv",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mX5jrV88hvN",
        "outputId": "71bccc2e-b18c-4e42-988b-7d710457e1b0"
      },
      "id": "6mX5jrV88hvN",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 15s 58ms/step - loss: 2.7034\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.9718\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 1.6975\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.5404\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.4446\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.3779\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.3248\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2803\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.2385\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.1991\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.1580\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.1159\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.0717\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.0247\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 0.9763\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 0.9248\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 14s 62ms/step - loss: 0.8728\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 14s 61ms/step - loss: 0.8201\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.7699\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 0.7227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "puz0Bo_z8k8L"
      },
      "id": "puz0Bo_z8k8L",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "vvHR9lZQ8puL"
      },
      "id": "vvHR9lZQ8puL",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LikVOLus8tMp",
        "outputId": "269ecb62-8a9b-4e42-a03a-7392596b378c"
      },
      "id": "LikVOLus8tMp",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "O sleep, never made no other memory.\n",
            "\n",
            "Boy:\n",
            "This untimely farlen is mine on the beast.\n",
            "\n",
            "LUCENTIO:\n",
            "Ah, now forsooth!\n",
            "\n",
            "VIRGILIA:\n",
            "I pray thee, madam.\n",
            "\n",
            "VOLUMNIA:\n",
            "I know you more, more.\n",
            "\n",
            "First Gentleman:\n",
            "And thou, or painting fair Sailorr, she weeps from death,\n",
            "But sweetest faste and labour's recomfortable champies;\n",
            "Where thou wilt, of what hast thou to her well?\n",
            "\n",
            "FERDINAND:\n",
            "Yes, Warwick, this is a pretty touch,--her thought of rain\n",
            "and beholding home, the earth he shall excuse\n",
            "The heart that tremblest which we had been\n",
            "Notice to eschange met, and what he hath\n",
            "perceive the body peril time.\n",
            "\n",
            "CAMILLO:\n",
            "O mistake, sir! here, sir.\n",
            "\n",
            "Clown:\n",
            "Sir, to return conscures have took too;\n",
            "But, like a tail hours of Vrencunst seen,\n",
            "Known fortune his most very wragged:\n",
            "Being so great and a man may stink\n",
            "Sitch'd wrath; I will tell her do you be so? and here will I leave you toly the\n",
            "Antigures; and then be convilated;\n",
            "So shall I not, 'tis subsequet the state: therefore, be too not;\n",
            "Yet even revolt, off. If, Tyr \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.410750389099121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEIJdk2b8wMR",
        "outputId": "d586d782-3e20-463f-906b-450aa7a22095"
      },
      "id": "oEIJdk2b8wMR",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nWhat hear't before is the rest? Three pounds well for the\\nlie, and these perpetutors found.\\n\\nMERCUTIO:\\nThou hast satisfected hostess: therefore, in my knoel-heart!\\nThou show'd us to our truncy dead to yourself.\\nThis last watce of God or blood, be there and\\nCherish yours with kind ear to his charge:\\nBut never mistake me not, my lord\\nThat Petruish'd By you did free upon:\\nThis letter how she's law we lie.\\n\\nLEONTES:\\nHapk, Clarence, and tell me of nothing:\\nbut I am not that there, by the way: for my tent\\nI have heready numbers go himself\\nUnto thy shoulder, thy love draw thy course,\\nSo finished his issue of their blood to\\nAs it was wont to hell and lengthen high Juliet:\\nAnd, for more, what may my house.\\n\\nMERCUTIO:\\nWhat mates and glad ob those joyful steps?\\nIf Catesby protect me or six,\\nAnd leave this land, thou hast been man,\\nNew datch'd in submantain. Viltuous brings that ravels i' the taste.\\n\\nHORTENSIO:\\nNay, hag, I commend your worship came to this.\\n\\nFirst Murderer:\\nTake thou the rest,\\nAr\"\n",
            " b\"ROMEO:\\nThou alt of all twice that else\\nWould fetch devouch and take it up Richmond in the trable.\\nThese little things is place from thee, or noubly?\\n\\nCORIOLANUS:\\nAway!\\n\\nSecond Senator:\\nThe princess of the meanest haughter and thy day\\nThat you should quietly be freedomed.\\n\\nMOPSA:\\nWe stay, mark thee in purpose, teaches will.\\n\\nLADY ANNE:\\nFoul deed was there would I had gives his dignity;\\nAnd so it is the sun but dancing upon\\nThy tearing to't, or else he comes\\nTo tarmet in: the prince your sons seen'd you withal.\\nO hateful duilty! chasts thou hast drink'd him,\\nAnd all the rest, Corioli; clad AEnaxate, my master.\\n\\nDUKE VINCENTIO:\\nCome on, better any thing to be to-name?\\n\\nPOMPHEY:\\nBishop, father, for my duty to their king,\\nAnd not known it. A pox or quarien\\nAnd dost his father's lands.\\n\\nDORCAS:\\nWho made me? 'tis sir,\\nThat hap bath mine early, to savage means,\\nSo fearful pointly of England;\\nAnd, to be hanged, I warrant him.\\n\\nFirst Citizens:\\nIf I do so? Thou didst speak with perforce.\\n\\nSecond Gentle\"\n",
            " b\"ROMEO:\\nThe patricians of your honours might them begin.\\n\\nHENRY BOLINGBROKE:\\nRise; where he cities should bowh not\\nThat he will stop at our Petrainty deeds;\\nAnd shows assist impeach my cousin's gentle\\nFlower, what we are this thing that you would say,\\nI would this dark so-niered sun:\\nBeat too melciles him to the ward,\\nTo bleosed his events, true weight to drink a prisoner,\\nNot we, a shortly cheer. Bushy,\\nGo made me ask some such as you,\\nAs yon govern on tewming prickes thus\\nFifther of submits not quickly moved again,\\nAddirate the cause, and not lent fur mine eye\\nThan you Juft have the curstivat\\nviews. Perchance of this time,\\nYet in this heels, at wagits to men's taughter,\\nAnd was subjectstorn on the hollow ere\\nWhom thou wert blood, when men are at the heel,\\nit is, my lord, my lord, to leave up\\nThe twos and comfort of my life.\\nDear from the cause, a royal person's cap,\\nAnd by the frare can clouds them curds and rewards:\\nAnd he that smiles at middle-babties straight\\nMy prayers against thy lips!\"\n",
            " b\"ROMEO:\\nNot so near them,\\nYour penitence may be with thee: here I behind him,\\nWere he again? directions, for I hear off guest\\nFrom Oxford that she sits in heaven and\\nBy laughing the watch; 'tis twenty in this matter\\nBut that his good tongue, though such beats to\\nmore than next of great host to my steeds,\\nBut for my visage in thy love, these cords\\nLew approbation? You have all\\npoisonous-by\\ncontent their tender hate I move.\\n\\nFRIAR LAURENCE:\\nI saw some noble earls.\\n\\nShepherd:\\nTake heeds to wash;\\nFor most it confusing your wrongs:\\nMark'd you the executiona like him.\\n\\nLord:\\nThou art sccown, and fit hold\\nShow their hearts to thee at once;\\nTo chide upon your gape; and your father doth my treed to\\nher ready.\\n\\nDUKE VINCENTIO:\\nBy man, abode thy choice, thou knowist\\nThy day is ope. We did, my lord.\\n\\nKING HENRY VI:\\nFarewell, she says there is, condemnments have took\\nUnto a coxposity; and temptaying shall\\nyou shall entall for your throats there\\nto point what she wedless thou art.\\n\\nANGELO:\\nOne I had fought\"\n",
            " b\"ROMEO:\\nBring before the bud that matter, though she comes.\\n\\nWidow:\\nNirtt tell me, if you will think we may never roar\\nTill it be here foul deslition, Ifay\\nSay he comes.\\n\\nLord Master Garden me, or else thou wert\\nOf all exter'd. Who is't from me?\\n\\nDUKE VINCENTIO:\\nYour special curse is to him with thee, the\\nnews, i' faith, I know the cause were nursed here:\\nGod knows not so.\\n\\nDUKE OF YORK:\\nWho very near tunes I big us? Tell not me\\nSo long amovey the perfect day's hour\\nBring in revenge.\\n\\nCAMILLO:\\nThere did swear swent to be revenged not nor so far\\nThough he misture of her: he would\\nBury with the mirth, her father, I grow.\\n\\nDUKE VINCENTIO:\\nRomeo, there let him talk, and, in the disobellow\\nA traitor to the crown, my ghastly did,\\nDo they had her accustom'd two his\\nwriting with thee our five way or 't: yell,\\nSay so, night that I am bound to you: hence, ant the\\nbuy is like to do.\\n\\nLORD ROSS:\\nOne of these kented aims and extremes a\\nlittle glory.\\n\\nClown:\\nWhat say'st thou is not this meeting?\\n\\nPOLIXENES\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.1715967655181885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOZr6sQt8xs8",
        "outputId": "0191854d-3e3e-452f-8e3f-529388b3970f"
      },
      "id": "sOZr6sQt8xs8",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7b05ff52eef0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwqZq1YO8zgR",
        "outputId": "96186622-b6d2-4788-cbf6-9fb085ca168c"
      },
      "id": "HwqZq1YO8zgR",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "What, rusty gentleman? what hast thou wert not?\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Poor cousin, we are one of me marc\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}